{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5579b49",
   "metadata": {},
   "source": [
    "Q1. What is the main difference between the Euclidean distance metric and the Manhattan distance metric in KNN? How might this difference affect the performance of a KNN classifier or regressor?\n",
    "\n",
    "Ans1- The main difference between the Euclidean distance metric and the Manhattan distance metric in KNN is the way they calculate distance between two points. Euclidean distance is calculated as the square root of the sum of squared differences between corresponding components of two points. On the other hand, Manhattan distance is calculated as the sum of absolute differences between corresponding components of two points. The choice of distance metric can affect the performance of a KNN classifier or regressor depending on the structure of the data. In general, Euclidean distance works well when the data is continuous and Manhattan distance works well when the data is categorical.\n",
    "\n",
    "Q2. How do you choose the optimal value of k for a KNN classifier or regressor? What techniques can be used to determine the optimal k value?\n",
    "\n",
    "Ans2- The optimal value of k for a KNN classifier or regressor can be chosen using techniques such as cross-validation, grid search, and random search. Cross-validation involves dividing the data into training and validation sets, and then training the model using different values of k on the training set and evaluating the performance on the validation set. Grid search involves evaluating the performance of the model using different combinations of hyperparameters, including k, and choosing the combination that results in the best performance. Random search is similar to grid search, but instead of evaluating all possible combinations, it randomly samples a subset of hyperparameters and evaluates the performance.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q3. How does the choice of distance metric affect the performance of a KNN classifier or regressor? In what situations might you choose one distance metric over the other?\n",
    "\n",
    "Ans3- The choice of distance metric can affect the performance of a KNN classifier or regressor depending on the structure of the data. In general, Euclidean distance works well when the data is continuous and Manhattan distance works well when the data is categorical. In situations where the data contains both continuous and categorical features, a hybrid distance metric such as the Gower distance metric can be used.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q4. What are some common hyperparameters in KNN classifiers and regressors, and how do they affect the performance of the model? How might you go about tuning these hyperparameters to improve model performance?\n",
    "\n",
    "Ans4- Some common hyperparameters in KNN classifiers and regressors include the number of neighbors (k), the distance metric, and the method used to compute distances. These hyperparameters can affect the performance of the model, and tuning them can improve the model's accuracy. For example, increasing the value of k can reduce the impact of noise in the data, but can also make the decision boundary less sharp. The choice of distance metric can also affect the performance of the model, as discussed in the previous question. The method used to compute distances, such as brute force or tree-based methods, can also affect the performance of the model.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q5. How does the size of the training set affect the performance of a KNN classifier or regressor? What techniques can be used to optimize the size of \n",
    "the training set?\n",
    "\n",
    "Ans5-The size of the training set can affect the performance of a KNN classifier or regressor. As the size of the training set increases, the model becomes more accurate but also takes longer to compute. A common technique for optimizing the size of the training set is to use a validation curve, which involves training the model using different sizes of the training set and evaluating the performance on a validation set. The optimal size of the training set is the size that results in the best performance.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q6. What are some potential drawbacks of using KNN as a classifier or regressor? How might you overcome these drawbacks to improve the performance of the model?\n",
    "\n",
    "Ans6- Some potential drawbacks of using KNN as a classifier or regressor include the need to choose an appropriate value for k, the high computational cost of computing distances for large datasets, and the sensitivity of the model to irrelevant features. To overcome these drawbacks, techniques such as cross-validation and hyperparameter tuning can be used to choose an appropriate value of k. In addition, feature selection and dimensionality reduction techniques can be used to reduce the number of irrelevant features and improve the performance of the model. Finally, approximations and optimizations such as tree-based methods can be used to reduce the computational cost of KNN.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2c50ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
