{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9a6a714",
   "metadata": {},
   "source": [
    "Q1. What is Lasso Regression, and how does it differ from other regression techniques?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b75e8ce",
   "metadata": {},
   "source": [
    "Ans1-Lasso Regression, or Least Absolute Shrinkage and Selection Operator, is a linear regression technique that involves adding a penalty term to the regression objective function. The penalty term is the absolute sum of the coefficients of the regression model. Unlike other regression techniques such as ordinary least squares (OLS) regression, Lasso Regression performs both regularization and feature selection, which means it can automatically identify and eliminate irrelevant features from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58a8488",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "353becf5",
   "metadata": {},
   "source": [
    "Q2. What is the main advantage of using Lasso Regression in feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcf33fb",
   "metadata": {},
   "source": [
    "Ans2-The main advantage of using Lasso Regression in feature selection is that it can effectively identify and select only the most important features in the dataset, while discarding the rest. This is because the Lasso penalty shrinks the coefficients of the features towards zero, effectively removing the irrelevant features from the model. This can improve the model's performance by reducing overfitting and increasing its generalizability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e493d22e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f22f4a83",
   "metadata": {},
   "source": [
    "Q3. How do you interpret the coefficients of a Lasso Regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c8fd55",
   "metadata": {},
   "source": [
    "Ans3-The coefficients of a Lasso Regression model can be interpreted in the same way as those of a regular linear regression model. The sign of the coefficient indicates the direction of the relationship between the independent variable and the dependent variable, while the magnitude of the coefficient indicates the strength of the relationship. However, in Lasso Regression, the magnitude of the coefficient is also affected by the penalty term, which can shrink the coefficient towards zero or eliminate it entirely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb625c8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7deca535",
   "metadata": {},
   "source": [
    "Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the\n",
    "model's performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4858397c",
   "metadata": {},
   "source": [
    "Ans4-The main tuning parameter in Lasso Regression is the regularization parameter, which controls the strength of the penalty term in the objective function. The regularization parameter is denoted by lambda, and its value can be adjusted to balance the trade-off between model complexity and accuracy. A larger value of lambda will result in a more heavily penalized model, which can reduce the risk of overfitting but may also reduce the model's accuracy. Conversely, a smaller value of lambda will result in a less heavily penalized model, which may increase the model's accuracy but may also increase the risk of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f878ae7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c52af99c",
   "metadata": {},
   "source": [
    "Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db9ea41",
   "metadata": {},
   "source": [
    "Ans5-Lasso Regression is a linear regression technique, which means it can only model linear relationships between the independent and dependent variables. However, Lasso Regression can be used for non-linear regression problems by first transforming the independent variables using non-linear functions such as polynomials, logarithms, or exponentials. This will allow the model to capture non-linear relationships between the variables. Alternatively, other non-linear regression techniques such as polynomial regression or decision trees may be more appropriate for non-linear problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a9a76c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "401ddf1a",
   "metadata": {},
   "source": [
    "Q6. What is the difference between Ridge Regression and Lasso Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc556711",
   "metadata": {},
   "source": [
    "Ans6-Ridge Regression and Lasso Regression are both linear regression techniques that involve adding a penalty term to the regression objective function to prevent overfitting. However, they differ in the type of penalty term used. Ridge Regression adds a penalty term that is proportional to the square of the coefficients of the regression model, while Lasso Regression adds a penalty term that is proportional to the absolute value of the coefficients of the model. This results in different effects on the magnitude of the coefficients, with Ridge Regression shrinking the coefficients towards zero, while Lasso Regression can shrink some coefficients to exactly zero, effectively performing feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b02ba6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3eb2e1ef",
   "metadata": {},
   "source": [
    "Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b5f146",
   "metadata": {},
   "source": [
    "Ans7-Yes, Lasso Regression can handle multicollinearity in the input features by shrinking the coefficients of the correlated features towards zero. In Lasso Regression, the L1 penalty term encourages the coefficients of the less important features to be exactly zero, effectively performing feature selection. This means that the model automatically selects only the most important features, and the coefficients of the correlated features that are less important are shrunk towards zero. This is in contrast to Ridge Regression, which shrinks the coefficients of all features towards zero but does not result in exact feature selection. Therefore, Lasso Regression can be a better option than Ridge Regression when dealing with multicollinearity and feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b524eaaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63b0889e",
   "metadata": {},
   "source": [
    "Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9bae8d",
   "metadata": {},
   "source": [
    "Ans8- The choice of the regularization parameter, lambda, is critical in Lasso Regression because it controls the trade-off between fitting the data well and avoiding overfitting. The goal is to choose a lambda value that results in a model with good predictive performance while maintaining model simplicity.\n",
    "\n",
    "One common approach is to use cross-validation to select the optimal value of lambda. The data is divided into k subsets, and the model is trained on k-1 subsets and validated on the remaining subset. This process is repeated k times, with each subset being used for validation exactly once. The average validation error across the k iterations is computed for each value of lambda, and the value that results in the lowest error is chosen as the optimal value.\n",
    "\n",
    "Alternatively, some packages implement methods such as the Lasso path or the Bayesian Information Criterion (BIC) to select the optimal value of lambda. The Lasso path plots the coefficient values as a function of lambda, allowing the user to see which coefficients are shrunk to zero as lambda increases. The BIC method uses a penalty term that is a function of both the residual sum of squares and the number of nonzero coefficients in the model.\n",
    "\n",
    "It's important to note that the optimal value of lambda may vary depending on the specific dataset and the research question being investigated. Therefore, it's recommended to try a range of lambda values and choose the one that provides the best trade-off between model fit and simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e0672d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
