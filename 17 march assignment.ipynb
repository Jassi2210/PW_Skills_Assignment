{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63b006c5",
   "metadata": {},
   "source": [
    "## Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some algorithms that are not affected by missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b087df85",
   "metadata": {},
   "source": [
    "## Ans1- Missing values are a type of data issue that occurs when there is no value present for a particular variable or feature in a dataset. Missing values can occur for various reasons, such as data entry errors, measurement errors, or incomplete data. It is essential to handle missing values because they can negatively impact the quality of the analysis and the performance of machine learning algorithms.\n",
    "\n",
    "## Missing values can cause bias in statistical models and affect the accuracy of predictions. They can also lead to incorrect inferences and conclusions, as well as problems with data visualization. Therefore, it is crucial to handle missing values before performing any analysis or applying machine learning algorithms to the data.\n",
    "\n",
    "## Some of the machine learning algorithms that are not affected by missing values include decision trees, random forests, and gradient boosting machines. These algorithms can handle missing values by automatically creating rules to handle missing data or by using surrogate splits to substitute for missing values during the decision-making process. These algorithms can also be combined with imputation techniques to further improve their performance on datasets with missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06c1322",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "590bc699",
   "metadata": {},
   "source": [
    "## Q2: List down techniques used to handle missing data. Give an example of each with python code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a408b824",
   "metadata": {},
   "source": [
    "## Ans2- There are several techniques used to handle missing data in a dataset. Here are a few examples with Python code:\n",
    "\n",
    "## 1.Deletion: In this technique, we simply delete the rows or columns that have missing values. This is only recommended when the amount of missing data is small, and the analysis won't be significantly impacted by the loss of data.\n",
    "\n",
    "## 2.Imputation: In this technique, we replace missing values with an estimated value based on the available data.\n",
    "\n",
    "## 3.Forward or Backward fill: In this technique, we replace missing values with the most recently observed value (forward fill) or the next observed value (backward fill).\n",
    "\n",
    "## 4.Interpolation: In this technique, we estimate missing values based on the relationship between the available data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "574332e3",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5196\\1879650026.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Dropping rows with any missing values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Dropping columns with any missing values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Dropping rows with any missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Dropping columns with any missing values\n",
    "df.dropna(axis=1, inplace=True)\n",
    "\n",
    "# Simple imputation using mean\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df['column_name'] = imputer.fit_transform(df[['column_name']])\n",
    "\n",
    "# Forward fill\n",
    "df.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Backward fill\n",
    "df.fillna(method='bfill', inplace=True)\n",
    "\n",
    "# Interpolation using linear method\n",
    "df['column_name'] = df['column_name'].interpolate(method='linear')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5074afe2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "84a7d455",
   "metadata": {},
   "source": [
    "## Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bad60b1",
   "metadata": {},
   "source": [
    "## Ans3-Imbalanced data refers to a situation in which the classes or categories in a dataset are not represented equally. For example, in a binary classification problem, if one class has significantly fewer observations than the other, the data is said to be imbalanced. Imbalanced data is a common issue in many real-world datasets, such as fraud detection, disease diagnosis, and anomaly detection.\n",
    "\n",
    "## If imbalanced data is not handled, it can lead to biased and inaccurate results. The model will be trained to predict the majority class, resulting in poor performance for the minority class. This can lead to false negatives, false positives, or both, depending on the problem. For example, in a medical diagnosis task, if the model is not trained on an adequate number of positive samples, it may fail to detect a disease, leading to serious consequences for the patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0964a3f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "96218355",
   "metadata": {},
   "source": [
    "## Q4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down- sampling are required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c096646d",
   "metadata": {},
   "source": [
    "## Ans4- Up-sampling and down-sampling are two techniques used to address imbalanced data.\n",
    "\n",
    "## Upsampling involves increasing the number of instances in the minority class by creating new synthetic instances, while down-sampling involves reducing the number of instances in the majority class by removing some observations.\n",
    "\n",
    "## For example, let's consider a dataset that contains two classes: Class A (90 observations) and Class B (10 observations). This dataset is considered imbalanced as there are more observations for Class A compared to Class B.\n",
    "\n",
    "## In this scenario, we can use up-sampling to balance the dataset by creating new synthetic instances for Class B. One technique for doing this is called the Synthetic Minority Over-sampling Technique (SMOTE), which creates synthetic instances based on the nearest neighbors of the minority class. The resulting dataset may now contain 90 observations for Class A and 90 observations for Class B.\n",
    "\n",
    "## Alternatively, we can use down-sampling to balance the dataset by removing some observations from Class A. For example, we can randomly sample 10 observations from Class A and combine it with the 10 observations from Class B to form a balanced dataset of 20 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc1f749",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20d30aab",
   "metadata": {},
   "source": [
    "## Q5: What is data Augmentation? Explain SMOTE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06e8fe2",
   "metadata": {},
   "source": [
    "## Ans5- Data augmentation is a technique used to increase the size of a dataset by generating new samples from existing data. This is often used in machine learning when the dataset is small or imbalanced. Data augmentation techniques include image rotation, flipping, and zooming, as well as text manipulation and oversampling.\n",
    "\n",
    "## SMOTE (Synthetic Minority Over-sampling Technique) is a popular data augmentation technique used to balance imbalanced datasets. SMOTE works by creating synthetic examples of the minority class by interpolating between existing examples. This is done by selecting an example from the minority class and finding its k-nearest neighbors. Synthetic examples are then created by randomly selecting one of the neighbors and interpolating between the two examples. This process is repeated until the desired level of balance is achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d677ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82a7b48e",
   "metadata": {},
   "source": [
    "## Q6: What are outliers in a dataset? Why is it essential to handle outliers?\n",
    "## Ans6- Outliers are data points that lie far away from the majority of the data points in a dataset. These data points can be either extremely high or low compared to other data points in the same dataset. Outliers can be caused by errors in data collection, measurement errors, or may represent genuine extreme values in the data.\n",
    "\n",
    "## It is essential to handle outliers in a dataset because they can significantly affect the accuracy of statistical models, leading to misleading results. Outliers can skew the distribution of the data, leading to inaccurate estimates of central tendency and variance.\n",
    "\n",
    "## There are several methods to handle outliers in a dataset, including:\n",
    "\n",
    "## 1.Removing the outliers: In this method, the outliers are identified and removed from the dataset. However, this method can lead to a significant loss of data.\n",
    "\n",
    "## 2.Replacing the outliers: In this method, the outliers are identified and replaced with a suitable value. For example, the outliers can be replaced with the median or mean value of the data.\n",
    "\n",
    "## 3.Transforming the data: In this method, the data is transformed using mathematical functions such as logarithmic or exponential functions. This can help to reduce the impact of outliers on the data.\n",
    "\n",
    "## 4.Using robust statistical models: Robust statistical models are less sensitive to outliers and can provide more accurate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e75ca77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "31c2de38",
   "metadata": {},
   "source": [
    "## Q7: You are working on a project that requires analyzing customer data. However, you notice that some of the data is missing. What are some techniques you can use to handle the missing data in your analysis?\n",
    "## Ans7- There are several techniques that can be used to handle missing data in an analysis. Here are some commonly used techniques:\n",
    "\n",
    "# 1.Deletion: In this technique, the missing values are simply deleted from the dataset. There are two types of deletion techniques:\n",
    "\n",
    "## Listwise Deletion: In this technique, any row that contains a missing value is deleted from the dataset.\n",
    "## Pairwise Deletion: In this technique, only the missing values in a particular variable are deleted, and the remaining data is used for analysis.\n",
    "\n",
    "# 2.Imputation: In this technique, the missing values are replaced with some other values. There are several techniques for imputation:\n",
    "\n",
    "## Mean/Median/Mode Imputation: In this technique, the missing values are replaced with the mean, median or mode value of the non-missing values in that variable.\n",
    "## Regression Imputation: In this technique, a regression model is used to predict the missing values based on the values of other variables in the dataset.\n",
    "## KNN Imputation: In this technique, the missing values are replaced with the values of the k-nearest neighbors in the dataset.\n",
    "\n",
    "# 3.Advanced Imputation techniques: There are some advanced techniques such as Expectation Maximization (EM) algorithm, Multiple Imputations using Chained Equations (MICE), etc. that can be used to handle missing data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa98707b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1bc00820",
   "metadata": {},
   "source": [
    "## Q8: You are working with a large dataset and find that a small percentage of the data is missing. What are some strategies you can use to determine if the missing data is missing at random or if there is a pattern to the missing data?\n",
    "## Ans8- There are several strategies that can be used to determine if missing data is missing at random or if there is a pattern to the missing data. Some of these strategies are:\n",
    "\n",
    "## 1.Visual inspection: One strategy is to visually inspect the data using plots such as scatter plots or histograms. If there is no visible pattern to the missing data, it may be missing at random.\n",
    "\n",
    "## 2.Missingness tests: Another strategy is to conduct missingness tests to determine if the missing data is related to other variables in the dataset. For example, a chi-square test can be used to determine if the missing data is related to another categorical variable.\n",
    "\n",
    "## 3.Imputation: A third strategy is to impute the missing data using various imputation techniques such as mean imputation or regression imputation. If the imputed values are similar to the observed values, it may be an indication that the missing data is missing at random.\n",
    "\n",
    "## 4.Data collection process: Finally, it may be helpful to review the data collection process to determine if there are any factors that may have contributed to the missing data. For example, if data was only collected from a certain group of participants, this may explain why certain data is missing.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a897614a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7439c905",
   "metadata": {},
   "source": [
    "## Q9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the dataset do not have the condition of interest, while a small percentage do. What are some strategies you can use to evaluate the performance of your machine learning model on this imbalanced dataset?\n",
    "## Ans9- There are several strategies that can be used to evaluate the performance of a machine learning model on an imbalanced dataset:\n",
    "\n",
    "## 1.Confusion matrix: A confusion matrix can be used to evaluate the performance of a classification model. It shows the number of true positives, true negatives, false positives, and false negatives.\n",
    "\n",
    "## 2.ROC curve: A receiver operating characteristic (ROC) curve is a plot of the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings. It can be used to evaluate the performance of a binary classification model.\n",
    "\n",
    "## 3.Precision-Recall curve: A precision-recall curve is a plot of the precision (the number of true positives divided by the number of true positives plus false positives) against the recall (the number of true positives divided by the number of true positives plus false negatives) at various threshold settings.\n",
    "\n",
    "## 4.F1 score: The F1 score is the harmonic mean of precision and recall. It is a measure of a model's accuracy that takes into account both precision and recall.\n",
    "\n",
    "## 5.Stratified sampling: When splitting the data into training and testing sets, stratified sampling can be used to ensure that the minority class is represented in both sets.\n",
    "\n",
    "## 6.Resampling techniques: Resampling techniques such as oversampling the minority class (e.g. using SMOTE) or undersampling the majority class can be used to balance the dataset and improve the performance of the model.\n",
    "\n",
    "## 7.Cost-sensitive learning: Cost-sensitive learning is a technique that assigns different misclassification costs to different classes. In an imbalanced dataset, the cost of misclassifying the minority class may be higher than the cost of misclassifying the majority class. Cost-sensitive learning can be used to account for this imbalance and improve the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e55010",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f011199",
   "metadata": {},
   "source": [
    "## Q10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to balance the dataset and down-sample the majority class?\n",
    "## Ans10- To balance the dataset and down-sample the majority class, the following methods can be employed:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4198fc39",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5196\\4221840372.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Assuming \"df\" is the dataframe containing the unbalanced data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# \"satisfied\" is the majority class and \"unsatisfied\" is the minority class\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mmajority_class\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'satisfaction'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'satisfied'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mminority_class\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'satisfaction'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'unsatisfied'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "## 1.Random under-sampling: In this method, we randomly select a subset of the majority class data points equal to the number of minority class data points. \n",
    "## This can be done using the sample() function in Python's pandas library.\n",
    "\n",
    "# Assuming \"df\" is the dataframe containing the unbalanced data\n",
    "# \"satisfied\" is the majority class and \"unsatisfied\" is the minority class\n",
    "majority_class = df[df['satisfaction'] == 'satisfied']\n",
    "minority_class = df[df['satisfaction'] == 'unsatisfied']\n",
    "\n",
    "# Randomly select a subset of majority class data points\n",
    "majority_downsampled = majority_class.sample(n=len(minority_class), random_state=42)\n",
    "\n",
    "# Concatenate the minority class with the downsampled majority class\n",
    "balanced_data = pd.concat([majority_downsampled, minority_class])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f466fe38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.Cluster-based under-sampling: In this method, we use clustering algorithms to group similar data points together and then randomly select data points from each cluster. \n",
    "# This can be done using the KMeans clustering algorithm in Python's scikit-learn library.\n",
    "#Example code:\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Assuming \"df\" is the dataframe containing the unbalanced data\n",
    "# \"satisfied\" is the majority class and \"unsatisfied\" is the minority class\n",
    "majority_class = df[df['satisfaction'] == 'satisfied']\n",
    "minority_class = df[df['satisfaction'] == 'unsatisfied']\n",
    "\n",
    "# Create a KMeans clustering model\n",
    "kmeans = KMeans(n_clusters=len(minority_class), random_state=42)\n",
    "\n",
    "# Fit the model on the majority class data\n",
    "kmeans.fit(majority_class)\n",
    "\n",
    "# Get the cluster labels for the majority class data\n",
    "labels = kmeans.predict(majority_class)\n",
    "\n",
    "# Select a random data point from each cluster\n",
    "majority_downsampled = majority_class.groupby(labels).apply(lambda x: x.sample(n=1, random_state=42))\n",
    "\n",
    "# Concatenate the minority class with the downsampled majority class\n",
    "balanced_data = pd.concat([majority_downsampled, minority_class])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8069ee10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic minority over-sampling technique (SMOTE): In this method, we create synthetic data points for the minority class by interpolating between existing data points. \n",
    "# This can be done using the SMOTE algorithm in Python's imbalanced-learn library.\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Assuming \"df\" is the dataframe containing the unbalanced data\n",
    "# \"satisfied\" is the majority class and \"unsatisfied\" is the minority class\n",
    "majority_class = df[df['satisfaction'] == 'satisfied']\n",
    "minority_class = df[df['satisfaction'] == 'unsatisfied']\n",
    "\n",
    "# Create a SMOTE object\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Fit the SMOTE object on the minority class data\n",
    "minority_oversampled, _ = smote.fit_resample(minority_class.drop('satisfaction', axis=1), minority_class['satisfaction'])\n",
    "\n",
    "# Concatenate the majority class with the oversampled minority class\n",
    "balanced_data = pd.concat([majority_class, minority_oversampled])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1065270e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae84f4d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f13c7c0c",
   "metadata": {},
   "source": [
    "## Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a project that requires you to estimate the occurrence of a rare event. What methods can you employ to balance the dataset and up-sample the minority class?\n",
    "## Ans11- To balance an imbalanced dataset, up-sampling can be used for the minority class. Some methods that can be employed to up-sample the minority class are:\n",
    "\n",
    "## Random over-sampling: In this method, the minority class is randomly sampled with replacement to increase its size to match that of the majority class. This method can lead to overfitting if the minority class is significantly smaller than the majority class.\n",
    "\n",
    "## Synthetic Minority Over-sampling Technique (SMOTE): SMOTE creates synthetic samples by interpolating between the samples of the minority class. It randomly selects a minority class instance and computes the k nearest neighbors. Then, it creates new instances by interpolating between the chosen instance and its k nearest neighbors. SMOTE reduces the risk of overfitting and improves the generalization ability of the model.\n",
    "\n",
    "## Adaptive Synthetic Sampling (ADASYN): ADASYN is an extension of SMOTE. It generates synthetic samples for the minority class by computing the density distribution of the minority class and focusing on generating samples in areas that are difficult to learn. This method can improve the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ec0b7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
