{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b154ceba",
   "metadata": {},
   "source": [
    "Q1. What is the KNN algorithm?\n",
    "\n",
    "Ans1- The KNN (K-Nearest Neighbors) algorithm is a supervised machine learning algorithm used for both classification and regression tasks. It is a non-parametric and lazy learning algorithm that makes predictions by finding the K nearest data points in the training set and using their labels (for classification) or values (for regression) to make a prediction on a new data point.\n",
    "\n",
    "Q2. How do you choose the value of K in KNN?\n",
    "\n",
    "Ans2- The choice of K in KNN is important as it affects the performance of the model. Choosing a larger K will lead to smoother decision boundaries and a reduction in overfitting, but may result in decreased accuracy. Choosing a smaller K will result in more complex decision boundaries, increased overfitting, and may be more sensitive to noisy data.\n",
    "\n",
    "The value of K can be chosen by performing a grid search over a range of K values and choosing the value that yields the highest accuracy or lowest error on a validation set. Another approach is to use domain knowledge or a priori information to choose an appropriate value of K.\n",
    "\n",
    "Q3. What is the difference between KNN classifier and KNN regressor?\n",
    "\n",
    "Ans3- KNN classifier is used for classification tasks where the goal is to predict the class label of a new data point based on the class labels of its K nearest neighbors. KNN regressor, on the other hand, is used for regression tasks where the goal is to predict the value of a continuous target variable based on the values of its K nearest neighbors.\n",
    "\n",
    "Q4. How do you measure the performance of KNN?\n",
    "\n",
    "Ans4- The performance of KNN can be measured using classification accuracy (for classification tasks) or mean squared error (for regression tasks) on a test set. Other performance metrics such as precision, recall, F1 score, and R-squared can also be used depending on the specific problem.\n",
    "\n",
    "Q5. What is the curse of dimensionality in KNN?\n",
    "\n",
    "Ans5- The curse of dimensionality refers to the phenomenon where the performance of KNN deteriorates as the number of features (dimensions) in the data increases. This is because as the number of dimensions increases, the distance between data points becomes less meaningful, and the data becomes more sparse, making it difficult to find meaningful nearest neighbors.\n",
    "\n",
    "Q6. How do you handle missing values in KNN?\n",
    "\n",
    "Ans6 - KNN can handle missing values by either imputing the missing values using the mean, median or mode of the feature or by ignoring the data points with missing values during the distance calculation. Alternatively, more sophisticated imputation techniques such as k-nearest neighbor imputation or matrix completion can be used to impute missing values before applying KNN.\n",
    "\n",
    "Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for which type of problem?\n",
    "\n",
    "Ans7 -The performance of KNN classifier and regressor depends on the specific problem and the nature of the data. KNN classifier is generally better suited for problems with discrete class labels and a small number of features, whereas KNN regressor is better suited for problems with continuous target variables and a larger number of features. In general, the choice between KNN classifier and regressor depends on the specific problem and the nature of the data.\n",
    "\n",
    "Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?\n",
    "\n",
    "Strengths:\n",
    "\n",
    "* KNN is a simple algorithm that is easy to understand and implement.\n",
    "* It can work well on small datasets with low dimensionality.\n",
    "* KNN is a non-parametric algorithm that does not make any assumptions about the underlying data distribution.\n",
    "* It can be used for both classification and regression tasks.\n",
    "\n",
    "Weaknesses:\n",
    "\n",
    "* KNN can be computationally expensive and slow when the dataset is large or high-dimensional.\n",
    "* It is sensitive to irrelevant features or noisy data in the dataset.\n",
    "* The performance of KNN can be affected by the choice of distance metric and the value of K.\n",
    "\n",
    "These weaknesses can be addressed by:\n",
    "\n",
    "* Using dimensionality reduction techniques to reduce the dimensionality of the data.\n",
    "* Removing noisy data or irrelevant features from the dataset.\n",
    "* Choosing an appropriate distance metric and value of K using cross-validation.\n",
    "\n",
    "\n",
    "Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?\n",
    "\n",
    "Euclidean distance is the straight-line distance between two points in a Euclidean space. It is calculated as the square root of the sum of the squared differences between the corresponding coordinates of two points.\n",
    "\n",
    "Manhattan distance, also known as city-block distance or L1 distance, is the distance between two points measured along the axes at right angles. It is calculated as the sum of the absolute differences between the corresponding coordinates of two points.\n",
    "\n",
    "In KNN, the choice of distance metric can affect the performance of the algorithm. Euclidean distance is commonly used in KNN, but Manhattan distance can be more appropriate in some cases, especially when dealing with sparse datasets or data with outliers.\n",
    "\n",
    "\n",
    "Q10. What is the role of feature scaling in KNN?\n",
    "\n",
    "Feature scaling is an important step in KNN as it helps to normalize the data and ensure that each feature contributes equally to the distance calculation. If the features have different scales or units, the distance calculation can be dominated by the features with larger values, leading to inaccurate results.\n",
    "\n",
    "There are different methods of feature scaling that can be used in KNN, such as normalization and standardization. Normalization scales the data to a range of 0 to 1, while standardization scales the data to have zero mean and unit variance. The choice of feature scaling method depends on the characteristics of the dataset and the problem at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1bafe3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
