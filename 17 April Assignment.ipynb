{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45aab078",
   "metadata": {},
   "source": [
    "Q1. What is Gradient Boosting Regression?\n",
    "\n",
    "Ans1 - Gradient Boosting Regression is a machine learning algorithm that is used for regression problems. It is an extension of the Gradient Boosting algorithm and involves training a sequence of weak regression models, such as decision trees, in a stage-wise manner. Each model in the sequence is trained to predict the residual errors of the previous model, with the overall goal of minimizing the mean squared error between the predicted and actual values of the target variable.\n",
    "\n",
    "Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared.\n",
    "\n",
    "Ans2- \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "842051d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "X,y = make_classification(n_samples=1000, n_features=4,\n",
    "                         n_informative=2, n_redundant=0,\n",
    "                         random_state=0, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47d90fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "391551d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "classifier2=GradientBoostingClassifier()\n",
    "classifier2.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a68f619e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,accuracy_score,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b29ba47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[153  13]\n",
      " [  7 157]]\n",
      "0.9393939393939394\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.92      0.94       166\n",
      "           1       0.92      0.96      0.94       164\n",
      "\n",
      "    accuracy                           0.94       330\n",
      "   macro avg       0.94      0.94      0.94       330\n",
      "weighted avg       0.94      0.94      0.94       330\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred2=classifier2.predict(X_test)\n",
    "print(confusion_matrix(y_test,y_pred2))\n",
    "print(accuracy_score(y_test,y_pred2))\n",
    "print(classification_report(y_test,y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b10a9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55067b30",
   "metadata": {},
   "source": [
    "Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters.\n",
    "\n",
    "Ans3 - To optimize the performance of the model, you can experiment with different hyperparameters such as learning rate, number of trees, and tree depth. You can use grid search or random search to find the best hyperparameters. Here are the steps involved in grid search:\n",
    "\n",
    "* Define a range of hyperparameters to search over.\n",
    "* Use nested cross-validation to evaluate the performance of the model for each combination of hyperparameters.\n",
    "* Select the hyperparameters that give the best performance.\n",
    "\n",
    "Q4. What is a weak learner in Gradient Boosting?\n",
    "\n",
    "Ans4 - A weak learner in Gradient Boosting is a model that is only slightly better than random guessing. Examples of weak learners include decision trees with shallow depth or linear regression models with low complexity. The goal of Gradient Boosting is to iteratively improve the predictions of the weak learners by training subsequent models on the residual errors of the previous models.\n",
    "\n",
    "Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
    "\n",
    "Ans5 - The intuition behind Gradient Boosting is to combine the predictions of a sequence of weak learners in a way that minimizes the overall error. Each weak learner in the sequence is trained to predict the residual errors of the previous models, with the goal of minimizing the residual errors at each stage. By combining the predictions of the weak learners, the algorithm is able to capture more complex patterns in the data and improve the overall accuracy of the model.\n",
    "\n",
    "Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "\n",
    "Ans6 - The Gradient Boosting algorithm builds an ensemble of weak learners in a stage-wise manner. At each stage, a new weak learner is added to the ensemble and trained to predict the residual errors of the previous models. The predictions of the weak learners are then combined to produce the final prediction. The process is repeated for a fixed number of iterations or until the desired level of accuracy is achieved.\n",
    "\n",
    "\n",
    "Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?\n",
    "\n",
    "Ans7 - The mathematical intuition behind Gradient Boosting algorithm involves the following steps:\n",
    "\n",
    "* Define a loss function to measure the difference between the predicted values and the actual values of the target variable.\n",
    "\n",
    "* Train a weak learner on the training data and make predictions on the training set.\n",
    "\n",
    "* Calculate the residuals by subtracting the actual values from the predicted values.\n",
    "\n",
    "* Fit a new weak learner to the residuals and make predictions on the training set.\n",
    "\n",
    "* Add the predictions of the new weak learner to the predictions of the previous weak learners.\n",
    "\n",
    "* Repeat steps 3 to 5 until a stopping criterion is met.\n",
    "\n",
    "* Calculate the final prediction by adding the predictions of all the weak learners.\n",
    "\n",
    "* Calculate the gradient of the loss function with respect to the final prediction.\n",
    "\n",
    "* Update the final prediction by taking a step in the direction of the negative gradient, with a step size determined by the learning rate.\n",
    "\n",
    "* Repeat steps 2 to 9 for a predetermined number of iterations or until the performance on a validation set stops improving.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c4ecb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe543100",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a02804",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8e0c40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6b881a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306ae54f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29d0ad1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30314522",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca458dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9705e87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
