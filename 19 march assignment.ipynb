{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4825e0a",
   "metadata": {},
   "source": [
    "## Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application.\n",
    "## Ans1- Min-Max scaling, also known as normalization, is a data preprocessing technique used to rescale numeric features to a common range, usually between 0 and 1. This technique is useful when different features in a dataset have different scales or units, which can cause problems for certain machine learning algorithms that assume features are on the same scale.\n",
    "## x_scaled = (x - x_min) / (x_max - x_min)\n",
    "## Here's an example of how to use Min-Max scaling in data preprocessing:\n",
    "\n",
    "## Suppose we have a dataset of housing prices with two features: square footage (in square feet) and number of bedrooms. The square footage values range from 500 to 2000 square feet, and the number of bedrooms ranges from 1 to 4. We want to use this dataset to train a linear regression model to predict housing prices.\n",
    "\n",
    "## Before we can train the model, we need to preprocess the data to ensure that both features are on the same scale. We can apply Min-Max scaling to both features using the formula above.\n",
    "\n",
    "## Let's say the minimum and maximum values for square footage are 500 and 2000, respectively, and the minimum and maximum values for the number of bedrooms are 1 and 4, respectively. We can use these values to scale the data as follows\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d23c9d70",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'square_footage' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12960\\1463211466.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msquare_footage_scaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msquare_footage\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m2000\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mbedrooms_scaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbedrooms\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'square_footage' is not defined"
     ]
    }
   ],
   "source": [
    "square_footage_scaled = (square_footage - 500) / (2000 - 500)\n",
    "bedrooms_scaled = (bedrooms - 1) / (4 - 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d5b445",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d859f920",
   "metadata": {},
   "source": [
    "## Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application.\n",
    "## Ans2- The Unit Vector technique is another data preprocessing technique used to scale features. Unlike Min-Max scaling, which rescales features to a range of 0 to 1, Unit Vector scaling rescales each feature so that it has unit norm, or a length of 1.\n",
    "## x_unit_scaled = x / ||x||\n",
    "## Here's an example of how to use Unit Vector scaling in data preprocessing:\n",
    "\n",
    "## Suppose we have a dataset of movie ratings with two features: the number of stars (on a scale of 1 to 5) and the number of reviews. We want to use this dataset to train a k-nearest neighbors (KNN) model to recommend movies based on user input.\n",
    "\n",
    "## Before we can train the model, we need to preprocess the data to ensure that both features are on the same scale. We can apply Unit Vector scaling to both features using the formula above.\n",
    "\n",
    "## Let's say we have a movie with 4 stars and 500 reviews. We can use these values to scale the data as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1300bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [4, 500]\n",
    "x_unit_scaled = x / ||x|| = [0.008, 0.999]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38033d25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dea32af8",
   "metadata": {},
   "source": [
    "## Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application.\n",
    "## Ans3- PCA, or Principal Component Analysis, is a technique used in data analysis and machine learning to reduce the dimensionality of a dataset by finding a lower-dimensional representation that captures the most important features of the original data.\n",
    "\n",
    "## PCA works by identifying the directions of maximum variance in the data and projecting the data onto a smaller number of dimensions, called principal components, that capture most of this variance. Each principal component is a linear combination of the original features, with coefficients chosen to maximize the variance captured by that component.\n",
    "\n",
    "## Here's an example of how to use PCA in dimensionality reduction:\n",
    "\n",
    "## Suppose we have a dataset of images, each represented as a vector of pixel intensities. Each image is 28x28 pixels, so the original dataset has 784 features. We want to reduce the dimensionality of the dataset to make it easier to analyze and visualize.\n",
    "\n",
    "## We can apply PCA to the dataset as follows:\n",
    "\n",
    "## 1.Standardize the data: We first subtract the mean and divide by the standard deviation of each feature to standardize the data and ensure that all features have equal importance.\n",
    "\n",
    "## 2.Compute the covariance matrix: We compute the covariance matrix of the standardized data, which measures how much each pair of features varies together.\n",
    "\n",
    "## 3.Compute the principal components: We use a mathematical technique called eigendecomposition to compute the eigenvectors and eigenvalues of the covariance matrix. The eigenvectors correspond to the principal components of the data, and the eigenvalues indicate the amount of variance captured by each component.\n",
    "\n",
    "## 4.Choose the number of principal components: We choose the number of principal components to retain based on the amount of variance we want to preserve. For example, we might choose to retain the top 50 principal components, which capture 90% of the variance in the data.\n",
    "\n",
    "## 5.Project the data onto the principal components: We project the standardized data onto the chosen principal components to obtain a lower-dimensional representation of the data. Each new feature is a linear combination of the original features, weighted by the coefficients of the corresponding principal component.\n",
    "\n",
    "## After applying PCA, we have a new dataset with a smaller number of features that captures most of the variance in the original data. We can use this new dataset for further analysis, such as visualization or machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d1dc08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ae5ad74",
   "metadata": {},
   "source": [
    "## Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept.\n",
    "## Ans4-PCA is a technique used for both dimensionality reduction and feature extraction. In feature extraction, the goal is to transform the original features into a new set of features that better captures the important characteristics of the data. PCA can be used for feature extraction by using the principal components as the new features.\n",
    "\n",
    "## Here's an example of how to use PCA for feature extraction:\n",
    "\n",
    "## Suppose we have a dataset of images, each represented as a vector of pixel intensities. Each image is 28x28 pixels, so the original dataset has 784 features. We want to extract a set of features that captures the important characteristics of the images for a classification task.\n",
    "\n",
    "## ## 1.We can apply PCA to the dataset as follows:\n",
    "\n",
    "## 1.Standardize the data: We first subtract the mean and divide by the standard deviation of each feature to standardize the data and ensure that all features have equal importance.\n",
    "\n",
    "## 2.Compute the covariance matrix: We compute the covariance matrix of the standardized data, which measures how much each pair of features varies together.\n",
    "\n",
    "## 3.Compute the principal components: We use a mathematical technique called eigendecomposition to compute the eigenvectors and eigenvalues of the covariance matrix. The eigenvectors correspond to the principal components of the data, and the eigenvalues indicate the amount of variance captured by each component.\n",
    "\n",
    "## 4.Choose the number of principal components: We choose the number of principal components to retain based on the amount of variance we want to preserve. For example, we might choose to retain the top 50 principal components, which capture 90% of the variance in the data.\n",
    "\n",
    "## 5.Project the data onto the principal components: We project the standardized data onto the chosen principal components to obtain a lower-dimensional representation of the data. Each new feature is a linear combination of the original features, weighted by the coefficients of the corresponding principal component.\n",
    "\n",
    "## After applying PCA, we have a new set of features that captures the important characteristics of the images. We can use these features for a classification task, such as distinguishing between different types of objects in the images.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccb285d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c27a00f4",
   "metadata": {},
   "source": [
    "## Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data.\n",
    "## Ans5- To use Min-Max scaling for preprocessing the data in a recommendation system for a food delivery service, we would follow these steps:\n",
    "\n",
    "## ## Identify the numerical features: We would identify the numerical features in the dataset that need to be scaled. In this case, we have features such as price, rating, and delivery time that are all numerical.\n",
    "\n",
    "## Determine the range of each feature: We would determine the minimum and maximum values for each numerical feature in the dataset.\n",
    "\n",
    "## Apply Min-Max scaling: We would apply the Min-Max scaling formula to each numerical feature, which transforms each value to a range between 0 and 1. The formula is:\n",
    "\n",
    "## x_scaled = (x - min(x)) / (max(x) - min(x))\n",
    "\n",
    "## where x is the original value, min(x) is the minimum value for that feature, and max(x) is the maximum value for that feature.\n",
    "\n",
    "## For example, if the minimum price in the dataset is $5 and the maximum price is $25, and we have a restaurant with a price of  15, we would scale the price as follows:\n",
    "\n",
    "## price_scaled = ( 15 -  5) / ( 25 -  5) = 0.5\n",
    "\n",
    "## Use the scaled features in the recommendation system: We would use the scaled features in the recommendation system, as they provide a more standardized representation of the original data. For example, we might use the scaled features to calculate similarity between items or to make predictions about which items a user might like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53bbdc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a87a668a",
   "metadata": {},
   "source": [
    "## Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset.\n",
    "## Ans6- To use PCA to reduce the dimensionality of a dataset for a stock price prediction model, we would follow these steps:\n",
    "\n",
    "## 1.Standardize the data: We would first standardize the dataset by subtracting the mean and dividing by the standard deviation of each feature. This ensures that all features are on the same scale and have equal importance in the PCA.\n",
    "\n",
    "## 2.Compute the covariance matrix: We would compute the covariance matrix of the standardized data, which measures how much each pair of features varies together.\n",
    "\n",
    "## 3.Compute the principal components: We would use PCA to compute the eigenvectors and eigenvalues of the covariance matrix. The eigenvectors correspond to the principal components of the data, which capture the most important patterns of variation in the dataset. We would choose the top k eigenvectors that capture most of the variance in the data, where k is the number of principal components we want to retain.\n",
    "\n",
    "## 4.Project the data onto the principal components: We would project the standardized data onto the k principal components to obtain a lower-dimensional representation of the data. Each new feature is a linear combination of the original features, weighted by the coefficients of the corresponding principal component.\n",
    "\n",
    "## 5.Use the new features in the prediction model: We would use the k principal components as the new features in the stock price prediction model. This would reduce the dimensionality of the dataset and remove any irrelevant or redundant features that might not be important for predicting stock prices.\n",
    "\n",
    "## By using PCA to reduce the dimensionality of the dataset, we can simplify the model and make it more efficient while still capturing the most important patterns of variation in the data. Additionally, the reduced dimensionality can help avoid overfitting and improve the accuracy of the prediction model.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c8f76a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bcea021a",
   "metadata": {},
   "source": [
    "## Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1.\n",
    "## Ans7- To perform Min-Max scaling to transform the values in the dataset [1, 5, 10, 15, 20] to a range of -1 to 1, we can follow these steps:\n",
    "\n",
    "## 1. Determine the minimum and maximum values in the dataset. In this case, the minimum value is 1 and the maximum value is 20.\n",
    "\n",
    "## 2. Apply the Min-Max scaling formula, which transforms each value x to a new scaled value x_scaled within the range of -1 to 1. The formula is:\n",
    "\n",
    "## x_scaled = ((x - min(x)) / (max(x) - min(x))) * 2 - 1\n",
    "\n",
    "## where x is the original value, min(x) is the minimum value in the dataset, max(x) is the maximum value in the dataset, and x_scaled is the scaled value in the range of -1 to 1.\n",
    "\n",
    "## 3.Apply the formula to each value in the dataset:\n",
    "\n",
    "## For x=1, x_scaled = ((1-1)/(20-1))*2-1 = -1\n",
    "## For x=5, x_scaled = ((5-1)/(20-1))*2-1 = -0.6\n",
    "## For x=10, x_scaled = ((10-1)/(20-1))*2-1 = 0\n",
    "## For x=15, x_scaled = ((15-1)/(20-1))*2-1 = 0.6\n",
    "## For x=20, x_scaled = ((20-1)/(20-1))*2-1 = 1\n",
    "## Therefore, the Min-Max scaled values for the dataset [1, 5, 10, 15, 20] in the range of -1 to 1 are:\n",
    "\n",
    "## [-1, -0.6, 0, 0.6, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16437f05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "720d29c4",
   "metadata": {},
   "source": [
    "## Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?\n",
    "## Ans8- Performing feature extraction using PCA involves computing the eigenvectors and eigenvalues of the covariance matrix of the dataset, and then selecting a subset of the eigenvectors as the principal components to retain. The number of principal components to retain depends on the amount of variance we want to capture in the dataset and the trade-off between model complexity and accuracy.\n",
    "\n",
    "## In this case, the dataset contains five features: [height, weight, age, gender, blood pressure]. The number of principal components to retain would depend on the amount of variance we want to capture in the dataset. One approach to determining the number of principal components to retain is to look at the explained variance ratio, which tells us the proportion of the total variance in the dataset that is explained by each principal component.\n",
    "\n",
    "## We can perform PCA on the dataset to obtain the explained variance ratio for each principal component. Based on the explained variance ratio, we can then decide on the number of principal components to retain. As a general rule, we would want to retain enough principal components to capture at least 80% of the total variance in the dataset.\n",
    "\n",
    "## Without knowing the specific values of the dataset, it's difficult to determine the exact number of principal components to retain. However, a common practice is to retain the top two or three principal components, which would capture the most important patterns of variation in the dataset while still keeping the model simple and interpretable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90cc967",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
